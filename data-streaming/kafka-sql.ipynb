{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"Testing PySpark Example\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Specify the paths to your JAR files\n",
    "jars = \"/opt/bitnami/spark/jars/kafka-clients-3.3.0.jar,\" \\\n",
    "       \"/opt/bitnami/spark/jars/spark-sql-kafka-0-10_2.12-3.3.0.jar,\" \\\n",
    "       \"/opt/bitnami/spark/jars/spark-streaming-kafka-0-10_2.12-3.3.0.jar,\" \\\n",
    "       \"/opt/bitnami/spark/jars/spark-token-provider-kafka-0-10_2.12-3.3.0.jar\"\n",
    "\n",
    "# Create the Spark Session with the JARs included\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Streaming from Kafka\") \\\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \\\n",
    "    .config(\"spark.jars\", jars) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4) \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"kafka_broker_1:19092,kafka_broker_2:19093,kafka_broker_3:19094\"\n",
    "kafka_topic_name = \"names_topic\"\n",
    "\n",
    "\n",
    "# Construct a streaming DataFrame that reads from test-topic\n",
    "orders_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Printing Schema of orders_df: \")\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import findspark\n",
    "import time\n",
    "import os\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages ' \\\n",
    "                                        'org.apache.spark:spark-sql-kafka-0-10_2.12-3.3.0,' \\\n",
    "                                        'org.apache.spark:spark-token-provider-kafka-0-10_2.12-3.3.0' \\\n",
    "                                        'pyspark-shell '\n",
    "\n",
    "    print(\"Stream Data Processing Application Started ...\")\n",
    "    print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"PySpark Structured Streaming with Kafka and Message Format as JSON\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"kafka_broker_1:19092,kafka_broker_2:19093,kafka_broker_3:19094\"\n",
    "kafka_topic_name = \"names_topic\"\n",
    "\n",
    "\n",
    "# Construct a streaming DataFrame that reads from test-topic\n",
    "orders_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Printing Schema of orders_df: \")\n",
    "orders_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "orders_df1 = orders_df.selectExpr(\"CAST(value AS STRING)\", \"timestamp\")\n",
    "\n",
    "orders_schema = StructType() \\\n",
    "    .add(\"order_id\", StringType()) \\\n",
    "    .add(\"order_product_name\", StringType()) \\\n",
    "    .add(\"order_card_type\", StringType()) \\\n",
    "    .add(\"order_amount\", StringType()) \\\n",
    "    .add(\"order_datetime\", StringType()) \\\n",
    "    .add(\"order_country_name\", StringType()) \\\n",
    "    .add(\"order_city_name\", StringType()) \\\n",
    "    .add(\"order_ecommerce_website_name\", StringType())\n",
    "\n",
    "orders_df2 = orders_df1 \\\n",
    "    .select(from_json(col(\"value\"), orders_schema) \\\n",
    "            .alias(\"orders\"), \"timestamp\")\n",
    "\n",
    "orders_df3 = orders_df2.select(\"orders.*\", \"timestamp\")\n",
    "orders_df3.printSchema()\n",
    "\n",
    "# Simple aggregate - find total_order_amount by grouping country, city\n",
    "orders_df4 = orders_df3.groupBy(\"order_country_name\", \"order_city_name\")\\\n",
    "    .agg({'order_amount': 'sum'})\\\n",
    "    .select(\"order_country_name\", \"order_city_name\", col(\"sum(order_amount)\")\n",
    "            .alias(\"total_order_amount\"))\n",
    "\n",
    "print(\"Printing Schema of orders_df4: \")\n",
    "orders_df4.printSchema()\n",
    "\n",
    "# Write final result into console for debugging purpose\n",
    "orders_agg_write_stream = orders_df4\\\n",
    "    .writeStream.trigger(processingTime='5 seconds')\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .format(\"console\")\\\n",
    "    .start()\n",
    "\n",
    "orders_agg_write_stream.awaitTermination()\n",
    "print(\"Stream Data Processing Application Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define the Scala and Spark versions\n",
    "scala_version = '2.12.15'  # This matches the Scala version used by Spark\n",
    "spark_version = '3.3.0'    # Spark version from the provided configuration\n",
    "\n",
    "# Specify the packages needed for Kafka integration\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.2.0'\n",
    "]\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://172.19.0.3:7077\") \\\n",
    "    .appName(\"kafka-example\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Show the status of the Spark session to confirm it's alive\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Scala and Spark versions\n",
    "scala_version = '2.12.15'  # This matches the Scala version used in the provided JAR file\n",
    "spark_version = '3.3.0'  # Spark version from the provided configuration\n",
    "\n",
    "# Specify the packages needed for Kafka, Hadoop, AWS, and other dependencies\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',  # Kafka integration\n",
    "    'org.apache.kafka:kafka-clients:2.8.1',  # Kafka clients\n",
    "    'org.apache.hadoop:hadoop-aws:3.2.0',  # Hadoop AWS integration\n",
    "    'com.amazonaws:aws-java-sdk-s3:1.11.375',  # AWS SDK for S3\n",
    "    'org.apache.commons:commons-pool2:2.8.0'  # Commons Pool 2\n",
    "]\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://172.19.0.3:7077\") \\\n",
    "    .appName(\"kafka-example\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Show the status of the Spark session to confirm it's alive\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
